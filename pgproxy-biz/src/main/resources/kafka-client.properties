#Kafka Producer properties
producer.bootstrap.servers=${kafka.producer.broker.host}
acks=all
retries=3
batch.size=16384
linger.ms=500
key.serializer=org.apache.kafka.common.serialization.StringSerializer
value.serializer=org.apache.kafka.common.serialization.StringSerializer
block.on.buffer.full=true
buffer.memory=33554432

#Kafka Consumer properties
consumer.bootstrap.servers=${kafka.consumer.broker.host}
group.id=kafka-consumer-group
enable.auto.commit=true
auto.commit.interval.ms=1000
session.timeout.ms=30000
key.deserializer=org.apache.kafka.common.serialization.StringDeserializer
value.deserializer=org.apache.kafka.common.serialization.StringDeserializer

# These buffer sizes seem to be needed to avoid consumer switching to
# a mode where it processes one bufferful every 5 seconds with multiple
# timeouts along the way.
fetch.min.bytes=10
receive.buffer.bytes=10
max.partition.fetch.bytes=10
